\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\def\GroupID{G01} % <------- ENTER YOUR COMP 6321 group number here

\begin{document}
\title{News Classification Models}
\author{Giselle Martel ID26352936 \and Firas Sawan ID26487815}
\maketitle

\begin{abstract}
   Abstract here. Give an executive summary of your report: rough goal, rough methods, rough results. Usually no more than 200 words.
\end{abstract}

%------------------------------------------------------------------------
\section{Introduction}
\textcolor{blue}{State your goal by giving give an example of the data you're working with and an example of the kind of prediction you hope to achieve. Don't bother with a literature review of your subject area, but do mention any important sources you directly relied upon.}\\

The term “fake news” mainly refers to false or inaccurate information that is mistakenly or inadvertently created or spread. Fake news may have some hints of truth, but lack any contextualizing details. They may not include any verifiable facts or sources or may include basic verifiable facts that are written using deliberately inflammatory language that leaves out pertinent details or only presents one viewpoint. Our project aims to analyze and classify a list of news articles from 2 different datasets containing fake news and Real news entries. Entries in both datasets have a similar structure and are of the following form:   

\begin{center}
\includegraphics[width=\linewidth]{Latex_Report/report/dt_example.png}
\end{center}

The datasets were retrieved from a similar experiment conducted by (insert author here) however, the implementation details of the experiment have been thoroughly modified and expanded. The basic organization of the datasets include a title for each article, the text of the article, its subject and its date. Our project aims to classify words from these articles into two categories, namely a fake news category and a true news category by predicting the probability that a particular word is in present in either a fake or true news article. This classification and prediction is to be made using machine learning models that we have learned in class, namely a Support Vector Machine Classifier, a Random Forest Classifier, a Decision Tree Classifier, a Multinomial Naive Bayes Classifier, a Logistic regression Classifier, and A convolutional neural network. \\

The results we aim to achieve are in the form of detailed graphs and confusion matrices for each of the classifiers as well as a report of accuracy, percision and recall metrics. 


%------------------------------------------------------------------------
\section{Methodology \& Experimental Results}

\textcolor{blue}{Describe the important steps you took to achieve your goal, alongside experimental results that followed. If certain steps (preprocessing, extra features, etc.) turned out to be important for maximizing prediction performance, then try to mention how much benefit you observed with/without that feature.}

The first step we had to take to prepare for our experiment was to preprocess our data. This was an important step given that we needed maximize prediction performance by getting rid of empty data cells as well as formatting all entries in each column of both datasets in a similar manner. Preprocessing the data involved parsing both datasets, clearing out empty data cells, formatting the date columns in a uniform manner for all cells. It also involved assigning each dataset their respective labels of fake vs true news. Once this was done  both files were joined together to create one big dataset that we later used in our tokenization process. Tokenization was a necessary process that allowed us to extract all unique words from the joined dataset in preparation for splitting the data into training and testing components. We opted to split the data into a 70\% training set and a 30\% testing set as it seemed ideal to have our training data consisting of more than double that of the testing data. Following the split we generated a document-term matrix for each of the training and testing data and converted both the testing and training datasets into dataframes that we later used in our models. \\

The first model we worked on was the Multinomial Naive Bayesian model given that it is suitable for classification of discrete features such as the tokenized text in our preprocessed dataset. We started by performing a hyper parameter search that allowed us to choose the best accuracy to fit our data. We used the corresponding sklearn library to generate our model and then calculated the training and testing scores as well as the overfitting before we could plot our data. Next, we picked the best estimator we found and made the predictions, generated a confusion matrix and displayed a graph as shown in Figure~\ref{first_figure}. The model successfully achieved an accuracy of 92.33\% and a mean squared error of 27.7\% and a precision of 94.25\%. 

\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/NB/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/NB/scores_plot_NB.png}
   \end{center}
        \vspace*{-8mm}
        \caption{\label{first_figure}}
\end{figure}

Next, we worked on the Decision Tree classifier with the goal of creating a supervised learning model that predicts the correct label for a particular word by learning simple decision rules inferred from the training data features. A hyper parameter search using various max\_depth values was conducted to obtain the best accuracy possible and to help us in generating the training and testing accuracy scores. The model's overfitting value was also calculated and the best estimator was then used to generate the predictions and display the confusion matrix as well as the graph shown in Figure~\ref{second_figure} below. The model successfully achieved an accuracy of 99.22\%, a mean squared error of 8.79\% and a precision of 99.03\%.
 
\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/DT/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/DT/scores_plot.png}
   \end{center}
        \vspace*{-8mm}
        \caption{\label{second_figure}}
\end{figure}

Next, we worked on the Random Forest classifier that fits a number of decision tree classifiers on various sub-samples of our dataset and uses averaging to improve the predictive accuracy while also controlling over-fitting to a satisfying degree. A hyper parameter search was conducted to find the best maximum depth and number of estimator values that provide us with the highest possible accuracy for our model. We then calculated the training and testing score values and determined the overfitting before generating a confusion matrix, a plot for the scores of each estimator and a plot displaying the feature importances for the top 25 words all shown in Figure~\ref{third_figure} below. The model successfully achieved an accuracy of 94.4\%, a mean squared error of 23.6\% and a precision of 93.99\%.\\ 

\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/RF/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/RF/scores_plot.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/RF/feature_importances.png}
   \end{center}
        \vspace*{-5mm}
        \caption{\label{third_figure}}
\end{figure}

A Support Vector Machine model, which is a form of supervised learning, was then used to further help us in our quest for category prediction and classification. This model had an advantage of being memory efficient in the sense that it uses a subset of training points in the decision function called support vectors. In terms of the parameters used, we performed a hyper parameter search over 2 types of kernels, namely a Gaussian kernel of type 'rbf' and a linear kernel, as well as various values for C and gamma. This allowed us to obtain the best estimator to calculate the training and testing scores as well as to determine overfitting and perform prediction calculations. A plot for the scores of each estimator as well as a confusion matrix shown in Figure~\ref{fourth_figure} were generated to help us visualize our results. The model successfully achieved an accuracy of 90.9\%, a mean squared error of 30.04\% and a precision of 91.24\%.

\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/SVC/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/SVC/scores_plot.png}
   \end{center}
        \vspace*{-5mm}
        \caption{\label{fourth_figure}}
\end{figure}

Next, we worked on a Logistic Regression model that we used to predict the probability of our categorically dependent tokenized text. We used the corresponding Sklearn library to generate our model and fit our data to it using hyper parameters obtained via a hyper parameter search for values of C. The search allowed us to obtain the best possible accuracy, calculate the training and testing scores as well as to determine overfitting and perform prediction calculations. The model worked in such a way that it treated our tokenized data as binary variables that contain data coded as 1 (in our case Real) or 0 (in our case "fake") and used this data to make predictions. We subsequently generated a graph of each of the training and testing scores as well as a confusion matrix (Figure~\ref{fifth_figure}) that helped us visualize the result. The model successfully achieved an accuracy of 99.08\%, a mean squared error of 9.55\% and a precision of 99.14\%. \\

\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/LR/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/LR/scores_plot.png}
   \end{center}
        \vspace*{-5mm}
        \caption{\label{fifth_figure}}
\end{figure}


Finally, a neural network was constructed 

%------------------------------------------------------------------------
\section{Conclusions}

\textcolor{blue}{Summarize what you could and could not conclude based on your experiments.
The ”References” section (bibliography) is optional. If you cite any books, websites, or academic papers, then you can add them to bibliography.bib and cite them in this re- port. Otherwise delete the references section.}

\begin{table*}[!hb]\centering
   \begin{center}
   \begin{tabular}{|l|c|c|c|c|c|}
   \hline
    Classifier & Accuracy & Mean Squared Error & Precision & Recall & fscore\\
   \hline\hline
   Naive Bayes & 92.33\% & 27.7\% & 94.25\% & 90.87\% & 92.53\% \\
   Decision Tree & 99.22\% & 8.79\% & 99.03\% & 99.48\% & 99.26\% \\
   Random forest & 94.40\% & 23.60\% & 93.99\% & 95.41\% & 94.69\%\\
   Support Vector Machine & 90.90\% & 30.04\% & 91.24\% & 91.51\% & 91.38\% \\
   Logistic Regression & 99.08\% & 9.55\% & 99.14\% & 99.10\% & 99.13\% \\
   Central Neural Network & 0.00\% & 0.00\% & 0.00\% & 0.00\% & 0.00\% \\
   \hline
   \end{tabular}
   \end{center}
   \caption{Summary of Model Results\label{first_table}}
\end{table*}

{\small
\bibliographystyle{cvpr_bibstyle}
\bibliography{bibliography}
}


\newpage
\appendix


%-------------------------------------------------------------------------
\section*{Appendix: Extra Results (Optional)}

% DELETE THIS TEXT
\textcolor{blue}{If you want to include extra more detailed results that did not fit within the main report, include them here. Or, you can just delete this example section.}

%-------------------------------------------------------------------------
\section*{Appendix: Examples of \LaTeX}

% The 'dilde' in Table~\ref{} puts a no-break space prevents "Table" and "1" so that
% the table number does not get split onto a separate line.
% and it is good LaTeX practice to put tildes between 

% DELETE THIS TEXT

This is a reference to Table~\ref{first_table} and Table~\ref{second_table}.
This is a reference to Figure~\ref{first_figure} and Figure~\ref{second_figure}.
This is a citation \cite{breiman2001statistical} and this is
multiple citations \cite{breiman2001statistical,bishop2006pattern}.
This is \textit{italics} and \textbf{bold} text.
This is a formula $\sum_{i=1}^N (y_i - \hat{y}_i(\mathbf{x}))^2$
that is inline with the text (`text style') and this is a



\end{document}
