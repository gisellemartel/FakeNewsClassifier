\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\def\GroupID{G01} % <------- ENTER YOUR COMP 6321 group number here

\begin{document}
\title{News Classification Models}
\author{Giselle Martel ID26352936 \and Firas Sawan ID26487815}
\maketitle

\begin{abstract}
   Intentionally deceptive content presented under the disguise of legitimate facts has become a widespread information accuracy and integrity problem affecting various aspects of modern day life. The following report attempts to employ supervised and unsupervised machine learning techniques to classify a collection of articles as "Fake" or "Real" news. Data was collected from the internet and was manipulated during a preprocessing phase to achieve an ideal input for the various models implemented to help with such classification. The following report compares the general accuracy and performance of a Logistic regression classifier, Random Forest Classifier, Decision Tree Classifier, Multinomial Naive Bayes Classifier and a Support Vector Machine classifier all of which are classified under the category of supervised learning. The report also implements a Central Neural Network.... The results show that Logistic regression has the highest level of accuracy and lowest level of overfitting, thus it was generally the best model to use for classifying articles into the two categories. This classifier was followed in terms of accuracy and performance by the other 4 classifiers in the order mentioned above and the results are presented in the following report. 
\end{abstract} 

%------------------------------------------------------------------------
\section{Introduction}
\textcolor{blue}{State your goal by giving give an example of the data you're working with and an example of the kind of prediction you hope to achieve. Don't bother with a literature review of your subject area, but do mention any important sources you directly relied upon.}\\

The term “fake news” mainly refers to false or inaccurate information that is mistakenly or inadvertently created or spread. Fake news may have some hints of truth, but lack any contextualizing details. They may not include any verifiable facts or sources or and may contain deliberately inflammatory language that leaves out pertinent details or only presents one viewpoint. Our project aims to analyze and classify a list of news articles from 2 different datasets containing fake news and Real news entries. Entries in both datasets have a similar structure and are of the following form:   

\begin{center}
\includegraphics[width=\linewidth]{Latex_Report/report/dt_example.png}
\end{center}

The datasets were retrieved from a similar experiment conducted by \textcolor{red}{(insert author here)} however, the implementation details of the experiment have been thoroughly modified and expanded. The basic organization of the datasets include a title for each article, the text of the article, its subject and its date. Our project aims to classify words from these articles into two categories, namely a fake news category and a true news category by predicting the probability that a particular word is in present in either a fake or true news article. This classification and prediction is to be made using supervised and unsupervised machine learning models that we have learned in class, namely a Logistic regression classifier, Random Forest Classifier, Decision Tree Classifier, Multinomial Naive Bayes Classifier and a Support Vector Machine classifier. Our goal is to present the respective results from each model and compare their performance to one another. The results we aim to achieve are in the form of detailed graphs and confusion matrices for each of the classifiers as well as a report of accuracy, precision and recall metrics. 


%------------------------------------------------------------------------
\section{Methodology \& Experimental Results}

\textcolor{blue}{Describe the important steps you took to achieve your goal, alongside experimental results that followed. If certain steps (preprocessing, extra features, etc.) turned out to be important for maximizing prediction performance, then try to mention how much benefit you observed with/without that feature.}

The first step we had to take to prepare for our experiment was to preprocess our data. This was an important step given that we needed maximize prediction performance by getting rid of empty data cells as well as formatting all entries in each column of both datasets in a similar manner. Preprocessing the data involved parsing both datasets, clearing out empty data cells and formatting the date columns in a uniform manner for all cells. It also involved assigning each dataset their respective labels of fake vs true news. Once this was done, both files were joined together to create one big dataset that we later used in our tokenization process. Tokenization was a necessary process that allowed us to extract all unique words from the joined dataset while excluding some stop-words that were present in abundance throughout the data. Following tokenization the data was split into training and testing components. We opted to split the data into a 70\% training set and a 30\% testing set as it seemed ideal to have our training data consisting of more than double that of the testing data. Following the split we generated a score graph, a document-term matrix for each of the training and testing data and converted both the testing and training datasets into dataframes that we later used in our models. \\

The first model we worked on was the Logistic Regression classifier that we used to predict the probability of our categorically dependent tokenized text. We used the corresponding Sklearn library to generate our model and fit our data to it using hyper parameters obtained via a hyper parameter search for values of C with the help of GridSearchCV method. The search allowed us to obtain the best possible accuracy, calculate the training and testing scores as well as to determine overfitting and perform prediction calculations. The model worked in such a way that it treated our tokenized data as binary variables that contain data coded as 1 (in our case Real) or 0 (in our case "fake") and used this data to make predictions. We subsequently generated a graph of each of the training and testing scores as well as a confusion matrix (Figure~\ref{first_figure}) that helped us visualize the result. The model successfully achieved an accuracy of 91.92\%, a mean squared error of 8.07\% and a precision of 92.70\%. \\

\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/LR/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/LR/scores_plot.png}
   \end{center}
        \vspace*{-5mm}
        \caption{\label{first_figure}}
\end{figure}

Next, we worked on the Random Forest classifier that fits a number of decision tree classifiers on various sub-samples of our dataset and uses averaging to improve the predictive accuracy while also controlling over-fitting to a satisfying degree. A hyper parameter search using GridSearchCV was conducted to find the best maximum depth and number of estimator values that provide us with the highest possible accuracy for our model. We then calculated the training and testing score values and determined the overfitting before generating a confusion matrix, a plot for the scores of each estimator and a plot displaying the feature importances for the top 25 words all shown in Figure~\ref{Second_figure} below. The model successfully achieved an accuracy of 90.48\%, a mean squared error of 9.51\% and a precision of 90.45\%.\\ 

\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/RF/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/RF/scores_plot.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/RF/feature_importances.png}
   \end{center}
        \vspace*{-5mm}
        \caption{\label{Second_figure}}
\end{figure}

Decision Tree classifier was the third model constructed with the goal of creating a classifier that predicts the correct label for a particular word by learning simple decision rules inferred from the training data features. A hyper parameter search using GridSearchCV and various max\_depth values was conducted to obtain the best accuracy possible and to help us in generating the training and testing accuracy scores. The model's overfitting value was also calculated and the best estimator was then used to generate the predictions and display the confusion matrix as well as the score graph shown in Figure~\ref{Third_figure} below. The model successfully achieved an accuracy of 87.54\%, a mean squared error of 12.45\% and a precision of 87.19\%.
 
\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/DT/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/DT/scores_plot.png}
   \end{center}
        \vspace*{-8mm}
        \caption{\label{Third_figure}}
\end{figure}

Multinomial Naive Bayesian model was the next to be constructed given that it is suitable for classification of discrete features such as the tokenized text in our preprocessed dataset. We started by using GridSearchCV to perform a hyper parameter search that allowed us to choose the best accuracy to fit our data. We used the corresponding sklearn library to generate our model and then calculated the training and testing scores as well as the overfitting before we could plot our data. Next, we picked the best estimator we found and made the predictions, generated a confusion matrix and displayed a score graph as shown in Figure~\ref{fourth_figure}. The model successfully achieved an accuracy of 88.35\% and a mean squared error of 11.64\% and a precision of 89.48\%. 

\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/NB/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/NB/scores_plot.png}
   \end{center}
        \vspace*{-8mm}
        \caption{\label{fourth_figure}}
\end{figure}

A Support Vector Machine model, which is a form of supervised learning, was then used to further help us in our quest for category prediction and classification. This model had an advantage of being memory efficient in the sense that it uses a subset of training points in the decision function called support vectors. In terms of the parameters used, we performed a hyper parameter search over 2 types of kernels, namely a Gaussian kernel of type 'rbf' and a linear kernel, as well as various values for C and gamma. This allowed us to obtain the best estimator to calculate the training and testing scores as well as to determine overfitting and perform prediction calculations. A plot for the scores of each estimator as well as a confusion matrix shown in Figure~\ref{fifth_figure} were generated to help us visualize our results. The model successfully achieved an accuracy of 83.29\%, a mean squared error of 16.70\% and a precision of 85.81\%.

\begin{figure}[h]
   \begin{center}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/SVC/confusion_matrix.png}
        \includegraphics[width=\linewidth]{Latex_Report/report/Graphs/SVC/scores_plot.png}
   \end{center}
        \vspace*{-5mm}
        \caption{\label{fifth_figure}}
\end{figure}

Finally, a neural network was constructed 

%------------------------------------------------------------------------
\section{Conclusions}

\textcolor{blue}{Summarize what you could and could not conclude based on your experiments.
The ”References” section (bibliography) is optional. If you cite any books, websites, or academic papers, then you can add them to bibliography.bib and cite them in this re- port. Otherwise delete the references section.}

As discussed in the previous section our quest for article classification involved training various types of classifiers each taking their own set of parameters to achieve the most ideal results possible. We notice that logistic regression had the highest accuracy at 91.92\% when it came to correctly classifying our tokenized data into the Fake and Real categories. The confusion matrix generated for this model shows that it correctly classified 5916 words into the "Real" category and 6458 words into the "Fake" category and overfitting. The degree of overfitting for this classifier was also the lowest obtained at 0.018. This allows us to conclude that logistic regression classifier was the best model to use on our preprocessed data. 

Logistic regression was closely followed by the Random forest classifier with a classification accuracy of 90.48\%. The confusion matrix generated for this classifier shows very similar results to that of the logistic regression with 5745 words correctly classified into the "Real" category and 6435 words into the "Fake" category. The model, however, had a much higher overfitting value at 0.705. This means that although the classifiers were very close in terms of their accuracy, logistic regression remains at an advantage given that it had much less overfitting of data. 

The Multinomial Naive Bayesian model comes in third place in terms of performance with a classification accuracy of 88.35\%. Its confusion matrix shows that the model was able to successfully classify 5696 words into the "Real" category and 6128 words into the "Fake" category. The model has an advantage over the other two that it was the fastest to run and classify the data with a very low degree of overfitting at 0.045. In total the model took approximately 3 seconds to run from start to finish including the hyper parameter search that occupies the bulk of execution time. Perhaps this model would be a better fit to use compared to the logistic regression classifier if the goal is to achieve speed over accuracy. 

Decision Tree Classifier follows the Bayesian model closely with a classification accuracy of 87.54\%. Its confusion matrix shows that the mode was able to correctly classify 5501 words into the "Real" category and 6198 words into the "Fake" category. The model was significantly slower to run than the Bayesian model at 2.6 minutes vs the 3 seconds the Bayesian model took to classify our data. It also had a much higher overfitting value at 1.596 and thus we can conclude that the Naive Bayesian model is better employed to solve our particular problem on the provided dataset.  

Lastly the support vector machine classifier was the slowest to run out of all classifiers taking approximately 10.7 minutes to fully classify the data. It had the lowest classification accuracy of 83.29\% compared to other models and an overfitting value of 0.183. The model was successfully able to classify 5476 words into the "Real" category compared and 5736 into the "Fake" category. This information allows us to conclude that although the overfitting value for the model was relatively low, that advantage is outweighed by the slow run time for the model and a logistic regression model can achieve better results at a faster pace and with less overfitting. 


\begin{table*}[!hb]\centering
   \begin{center}
   \begin{tabular}{|l|c|c|c|c|c|}
   \hline
    Classifier & Accuracy & Mean Squared Error & Precision & Recall & fscore\\
   \hline\hline
   Naive Bayes & 92.33\% & 27.7\% & 94.25\% & 90.87\% & 92.53\% \\
   Decision Tree & 99.22\% & 8.79\% & 99.03\% & 99.48\% & 99.26\% \\
   Random forest & 94.40\% & 23.60\% & 93.99\% & 95.41\% & 94.69\%\\
   Support Vector Machine & 90.90\% & 30.04\% & 91.24\% & 91.51\% & 91.38\% \\
   Logistic Regression & 99.08\% & 9.55\% & 99.14\% & 99.10\% & 99.13\% \\
   Central Neural Network & 0.00\% & 0.00\% & 0.00\% & 0.00\% & 0.00\% \\
   \hline
   \end{tabular}
   \end{center}
   \caption{Summary of Model Results\label{first_table}}
\end{table*}

{\small
\bibliographystyle{cvpr_bibstyle}
\bibliography{bibliography}
}


\newpage
\appendix


%-------------------------------------------------------------------------
\section*{Appendix: Extra Results (Optional)}

% DELETE THIS TEXT
\textcolor{blue}{If you want to include extra more detailed results that did not fit within the main report, include them here. Or, you can just delete this example section.}

%-------------------------------------------------------------------------
\section*{Appendix: Examples of \LaTeX}

% The 'dilde' in Table~\ref{} puts a no-break space prevents "Table" and "1" so that
% the table number does not get split onto a separate line.
% and it is good LaTeX practice to put tildes between 

% DELETE THIS TEXT

This is a reference to Table~\ref{first_table} and Table~\ref{second_table}.
This is a reference to Figure~\ref{first_figure} and Figure~\ref{second_figure}.
This is a citation \cite{breiman2001statistical} and this is
multiple citations \cite{breiman2001statistical,bishop2006pattern}.
This is \textit{italics} and \textbf{bold} text.
This is a formula $\sum_{i=1}^N (y_i - \hat{y}_i(\mathbf{x}))^2$
that is inline with the text (`text style') and this is a



\end{document}
